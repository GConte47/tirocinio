{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-27T16:48:20.577548Z",
     "start_time": "2025-03-27T16:48:17.151319Z"
    }
   },
   "source": "import pandas as pd",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:48:20.697068Z",
     "start_time": "2025-03-27T16:48:20.591922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#import annotazioni e gold standard\n",
    "annotazioni= pd.read_csv('../files/annotazioni_full.csv', delimiter=',')\n",
    "GS= pd.read_csv('../files/gold_standard.csv', delimiter=',')"
   ],
   "id": "c978589cc15f7dda",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# descrizione problema",
   "id": "f10a5d60f1447f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:48:21.369303Z",
     "start_time": "2025-03-27T16:48:21.357138Z"
    }
   },
   "cell_type": "code",
   "source": "annotazioni.columns",
   "id": "7e8a124fb2ea7bd0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TipoDiAnnotazione', 'ontologies', 'SOURCE', 'AttributoNomeCompleto',\n",
       "       'entity_id', 'pretty_name', 'cui', 'annotator'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:48:21.404255Z",
     "start_time": "2025-03-27T16:48:21.395611Z"
    }
   },
   "cell_type": "code",
   "source": "GS.columns",
   "id": "97e541465ed0e781",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TipoDiAnnotazione', 'Ontologies', 'Source', 'AttributoNomeCompleto',\n",
       "       'entity_id', 'pretty_name', 'cui'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:48:21.455560Z",
     "start_time": "2025-03-27T16:48:21.440089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#allineazione nomi\n",
    "annotazioni = annotazioni.rename(columns={'ontologies':'Ontologies','SOURCE':'Source','entity_id':'EntityID','pretty_name':'PrettyName','cui':'Cui','annotator':'Annotator',})\n",
    "annotazioni['Ontologies'] = annotazioni['Ontologies'].replace('SNOMEDCT', 'SNOMED-CT')\n",
    "\n",
    "GS = GS.rename(columns={'entity_id':'EntityID','pretty_name':'PrettyName','cui':'Cui','annotator':'Annotator',})"
   ],
   "id": "e14b9ff79f23ed28",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:48:21.532626Z",
     "start_time": "2025-03-27T16:48:21.509053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "set_gold = set(GS[['Ontologies', 'Cui']].itertuples(index=False, name=None))\n",
    "set_full = set(annotazioni[['Ontologies', 'Cui']].itertuples(index=False, name=None))\n",
    "\n",
    "# Calcoliamo i valori richiesti\n",
    "Nboth = len(set_gold & set_full)  # Presenti in entrambi\n",
    "Nleft = len(set_gold - set_full)  # Solo in df_gold\n",
    "Nright = len(set_full - set_gold)  # Solo in df_full\n",
    "\n",
    "print(f\"Nboth: {Nboth}, Nleft: {Nleft}, Nright: {Nright}\")"
   ],
   "id": "10a36a84dc5fa27d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nboth: 58, Nleft: 34, Nright: 1345\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:48:21.634406Z",
     "start_time": "2025-03-27T16:48:21.579367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#dettagliare questa analisi rispetto alle diverse ontologie\n",
    "\n",
    "set_GS = set(GS[['Ontologies', 'Cui']].itertuples(index=False, name=None))\n",
    "set_annotazioni = set(annotazioni[['Ontologies', 'Cui']].itertuples(index=False, name=None))\n",
    "\n",
    "# Unione dei due DataFrame per analisi per Ontology\n",
    "df_combined = pd.concat([GS.assign(source='GS'), annotazioni.assign(source='annotazioni')])\n",
    "\n",
    "# Creiamo una tabella che conta Nleft, Nright e Nboth per ogni Ontology\n",
    "ontology_counts = df_combined.groupby(['Ontologies', 'source']).size().unstack(fill_value=0)\n",
    "ontology_counts.columns = ['Nleft', 'Nright']\n",
    "ontology_counts['Nboth'] = ontology_counts.min(axis=1)  # Il minimo tra Nleft e Nright dà Nboth\n",
    "\n",
    "# Output finale\n",
    "print(ontology_counts)"
   ],
   "id": "a87f0284fc3c31f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Nleft  Nright  Nboth\n",
      "Ontologies                      \n",
      "LOINC           0    7903      0\n",
      "SNOMED-CT     127   20398    127\n",
      "UMLS          196    9801    196\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:48:21.679711Z",
     "start_time": "2025-03-27T16:48:21.646130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#dettagliare questa analisi rispetto alle diverse annotazioni\n",
    "# Aggiungiamo un annotatore fittizio per GS\n",
    "GS['Annotator'] = 'Gold_Standard'\n",
    "\n",
    "# Cambiamo il nome della colonna 'source' in 'Provenienza'\n",
    "GS['Provenienza'] = 'GS'\n",
    "annotazioni['Provenienza'] = 'annotazioni'\n",
    "df_combined = pd.concat([GS, annotazioni])\n",
    "\n",
    "# Raggruppiamo per Ontology e Annotator\n",
    "ontology_annotator_counts = df_combined.groupby(['Ontologies', 'Annotator', 'Provenienza']).size().unstack(fill_value=0)\n",
    "\n",
    "# Rinominiamo le colonne\n",
    "ontology_annotator_counts.columns = ['Nleft', 'Nright']\n",
    "ontology_annotator_counts['Nboth'] = ontology_annotator_counts.min(axis=1)\n",
    "\n",
    "# Output finale\n",
    "print(ontology_annotator_counts)"
   ],
   "id": "f35510cd5da10dfc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Nleft  Nright  Nboth\n",
      "Ontologies Annotator                          \n",
      "LOINC      Bioportal          0    7903      0\n",
      "SNOMED-CT  Bioportal          0    4791      0\n",
      "           Gold_Standard    127       0      0\n",
      "           MedCAT             0   15607      0\n",
      "UMLS       Gold_Standard    196       0      0\n",
      "           MedCAT             0    9801      0\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Valutazione della Qualità degli Annotatori",
   "id": "85a147f005ad95ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Metriche di valutazione",
   "id": "e192a161d0778319"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:52:41.181001Z",
     "start_time": "2025-03-27T16:52:41.164001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_annotation(gs_df, ann_df, column):\n",
    "\n",
    "    gs = set(gs_df[column].dropna())\n",
    "    ann = set(ann_df[column].dropna())\n",
    "\n",
    "    true_positives = len(gs & ann)  # Elementi corretti\n",
    "    precision = true_positives / len(ann) if len(ann) > 0 else 0\n",
    "    recall = true_positives / len(gs) if len(gs) > 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Precision\": round(precision, 4),\n",
    "        \"Recall\": round(recall, 4),\n",
    "        \"F1-score\": round(f1_score, 4)\n",
    "    }\n",
    "\n",
    "# Esempio di utilizzo\n",
    "\n",
    "\n",
    "metrics = evaluate_annotation(GS, annotazioni, \"AttributoNomeCompleto\")\n",
    "print(metrics)"
   ],
   "id": "90f9fdc865e71bf3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision': 0.0576, 'Recall': 0.5541, 'F1-score': 0.1043}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Metodi di confronto",
   "id": "333b879df4ca615b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:59:47.606163Z",
     "start_time": "2025-03-27T16:59:47.592577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def metodi_confronto(gs_df, ann_df, column):\n",
    "\n",
    "    gs = set(gs_df[column].dropna())\n",
    "    ann = set(ann_df[column].dropna())\n",
    "\n",
    "    intersection = gs & ann  # Concetti comuni\n",
    "\n",
    "    # Jaccard Similarity\n",
    "    jaccard = len(intersection) / len(gs | ann) if len(gs | ann) > 0 else 0\n",
    "\n",
    "    # Weighted Overlap\n",
    "    weighted_overlap = sum(1 for concept in intersection) / sum(1 for concept in gs) if len(gs) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Intersection\": len(intersection),\n",
    "        \"Jaccard Similarity\": round(jaccard, 4),\n",
    "        \"Weighted Overlap\": round(weighted_overlap, 4),\n",
    "        \"Valori dell'intersezione\": intersection\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "metrics = metodi_confronto(GS, annotazioni, \"AttributoNomeCompleto\")\n",
    "print(metrics)"
   ],
   "id": "328885d22be2c67b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Jaccard Similarity': 0.055, 'Weighted Overlap': 0.5541, 'Intersection': 41, \"Valori dell'intersezione\": {'person-gender_source_concept_id', 'condition_occurrence-provider_id', 'condition_occurrence-condition_source_concept_id', 'person-ethnicity_source_value', 'condition_occurrence-condition_type_concept_id', 'location-zip', 'location-address_1', 'condition_occurrence-condition_status_source_value', 'person-year_of_birth', 'location-address_2', 'condition_occurrence-stop_reason', 'person-race_source_concept_id', 'person-gender_source_value', 'condition_occurrence-person_id', 'location-county', 'person-person_id', 'condition_occurrence-condition_concept_id', 'location-city', 'person-ethnicity_concept_id', 'person-month_of_birth', 'location-location_id', 'location-state', 'location-location_source_value', 'condition_occurrence-condition_occurrence_id', 'person-ethnicity_source_concept_id', 'condition_occurrence-visit_detail_id', 'person-race_source_value', 'condition_occurrence-condition_end_date', 'condition_occurrence-condition_end_datetime', 'person-gender_concept_id', 'person-race_concept_id', 'condition_occurrence-visit_occurrence_id', 'person-day_of_birth', 'condition_occurrence-condition_start_datetime', 'person-location_id', 'person-person_source_value', 'condition_occurrence-condition_start_date', 'condition_occurrence-condition_status_concept_id', 'condition_occurrence-condition_source_value', 'person-care_site_id', 'person-provider_id'}}\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T17:20:13.811867Z",
     "start_time": "2025-03-27T17:20:13.726865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gs_df = GS.assign(source='GS')\n",
    "# Calcoliamo la frequenza d'uso dei concetti nel Gold Standard\n",
    "gs_freq = gs_df['AttributoNomeCompleto'].value_counts()\n",
    "\n",
    "\n",
    "# Mappiamo la frequenza come peso nei concetti di GS\n",
    "gs_df['Peso'] = gs_df['AttributoNomeCompleto'].map(gs_freq)\n",
    "\n",
    "\n",
    "# Uniamo i due DataFrame per ottenere i concetti comuni\n",
    "merged_df = pd.merge(gs_df, annotazioni, on='AttributoNomeCompleto', how='inner')\n",
    "\n",
    "# Calcoliamo l'overlap ponderato sommando i pesi dei concetti comuni\n",
    "overlap_ponderato = (merged_df['Peso'] * gs_freq[merged_df['AttributoNomeCompleto']]).sum()\n",
    "\n",
    "print(f'Overlap Ponderato: {overlap_ponderato}')\n"
   ],
   "id": "e8981cfebaa8e691",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap Ponderato: 0.0\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "NON CAPISCO PERCHÈ L'OVERLAP PONDERATO SIA UGUALE A 0",
   "id": "9c878221db047eb0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T17:22:32.480247Z",
     "start_time": "2025-03-27T17:22:32.466894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Troviamo l'intersezione tra Gold Standard e Annotazioni\n",
    "common_concepts = set(GS['AttributoNomeCompleto']).intersection(set(annotazioni['AttributoNomeCompleto']))\n",
    "\n",
    "# 3. Calcoliamo la percentuale di concetti del Gold Standard presenti nelle Annotazioni\n",
    "total_concepts_gs = len(GS)\n",
    "correctly_annotated = len(common_concepts)\n",
    "\n",
    "# Percentuale di concetti correttamente annotati\n",
    "annotation_accuracy = (correctly_annotated / total_concepts_gs) * 100\n",
    "\n",
    "# 4. Valutiamo la bontà degli annotatori\n",
    "print(f'Concetti totali nel Gold Standard: {total_concepts_gs}')\n",
    "print(f'Concetti correttamente annotati: {correctly_annotated}')\n",
    "print(f'Accuratezza delle annotazioni: {annotation_accuracy:.2f}%')"
   ],
   "id": "e470e527729d04f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concetti totali nel Gold Standard: 323\n",
      "Concetti correttamente annotati: 41\n",
      "Accuratezza delle annotazioni: 12.69%\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Calcolo metriche su più attributi",
   "id": "c56fe77ca1bcb8fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T17:38:09.431146Z",
     "start_time": "2025-03-27T17:38:09.124234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_metrics(gs_df, ann_df):\n",
    "    # Inizializziamo un dizionario per memorizzare i risultati per ogni concetto\n",
    "    metrics = {'AttributoNomeCompleto': [], 'Precision': [], 'Recall': [], 'F1': []}\n",
    "\n",
    "    # Elenco dei concetti unici presenti nel GS\n",
    "    all_concepts = gs_df['AttributoNomeCompleto'].unique()\n",
    "\n",
    "    for concept in all_concepts:\n",
    "        # Filtra i concetti corrispondenti nel GS e nelle Annotazioni\n",
    "        gs_concepts = set(gs_df[gs_df['AttributoNomeCompleto'] == concept]['AttributoNomeCompleto'])\n",
    "        ann_concepts = set(ann_df[ann_df['AttributoNomeCompleto'] == concept]['AttributoNomeCompleto'])\n",
    "\n",
    "        # Calcolo della Precision\n",
    "        precision = len(gs_concepts.intersection(ann_concepts)) / len(ann_concepts) if len(ann_concepts) > 0 else 0\n",
    "\n",
    "        # Calcolo del Recall\n",
    "        recall = len(gs_concepts.intersection(ann_concepts)) / len(gs_concepts) if len(gs_concepts) > 0 else 0\n",
    "\n",
    "        # Calcolo del F1-Score\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        # Memorizza i risultati per ogni concetto\n",
    "        metrics['AttributoNomeCompleto'].append(concept)\n",
    "        metrics['Precision'].append(precision)\n",
    "        metrics['Recall'].append(recall)\n",
    "        metrics['F1'].append(f1)\n",
    "\n",
    "    # Convertiamo in DataFrame per una migliore visualizzazione\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "def calculate_aggregated_metrics(metrics_df):\n",
    "    # Macro-average: media delle metriche\n",
    "    precision_macro = metrics_df['Precision'].mean()\n",
    "    recall_macro = metrics_df['Recall'].mean()\n",
    "    f1_macro = metrics_df['F1'].mean()\n",
    "\n",
    "    # Micro-average: somma dei veri positivi, falsi positivi e falsi negativi\n",
    "    total_tp = sum([metrics_df.iloc[i]['Precision'] * metrics_df.iloc[i]['Recall'] for i in range(len(metrics_df))])\n",
    "    total_fp = sum([metrics_df.iloc[i]['Precision'] * (1 - metrics_df.iloc[i]['Recall']) for i in range(len(metrics_df))])\n",
    "    total_fn = sum([metrics_df.iloc[i]['Recall'] * (1 - metrics_df.iloc[i]['Precision']) for i in range(len(metrics_df))])\n",
    "\n",
    "    precision_micro = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall_micro = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    f1_micro = (2 * precision_micro * recall_micro) / (precision_micro + recall_micro) if (precision_micro + recall_micro) > 0 else 0\n",
    "\n",
    "    # Restituisce le metriche aggregate\n",
    "    aggregated_metrics = {\n",
    "        'Precisionmacro': precision_macro,\n",
    "        'Recallmacro': recall_macro,\n",
    "        'F1macro': f1_macro,\n",
    "        'Precisionmicro': precision_micro,\n",
    "        'Recallmicro': recall_micro,\n",
    "        'F1micro': f1_micro\n",
    "    }\n",
    "\n",
    "    return aggregated_metrics\n",
    "\n",
    "# Calcoliamo le metriche per ciascun concetto (AttributoNomeCompleto)\n",
    "metrics_df = calculate_metrics(gs_df, annotazioni)\n",
    "\n",
    "# Calcoliamo le metriche aggregate (Macro e Micro)\n",
    "aggregated_metrics = calculate_aggregated_metrics(metrics_df)\n",
    "\n",
    "# Visualizziamo le metriche aggregate\n",
    "print(\"Metriche Aggregate:\")\n",
    "for metric, value in aggregated_metrics.items():\n",
    "    print(f\"{metric}: {value}\")\n"
   ],
   "id": "efe28a1747d9e75a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metriche Aggregate:\n",
      "Precisionmacro: 0.5540540540540541\n",
      "Recallmacro: 0.5540540540540541\n",
      "F1macro: 0.5540540540540541\n",
      "Precisionmicro: 1.0\n",
      "Recallmicro: 1.0\n",
      "F1micro: 1.0\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analisi della Similarit`a tra Attributi con Annotazioni Diverse",
   "id": "a1239dfbd55d143c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T17:59:40.052558Z",
     "start_time": "2025-03-27T17:59:39.760321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Funzione per calcolare l'intersezione tra due insiemi\n",
    "def intersection(set1, set2):\n",
    "    return len(set1.intersection(set2))\n",
    "\n",
    "# Funzione per calcolare la similarità di Jaccard\n",
    "def jaccard_similarity(set1, set2):\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2)) if len(set1.union(set2)) > 0 else 0\n",
    "\n",
    "# Funzione per calcolare il coefficiente di Sorensen-Dice\n",
    "def sorensen_dice(set1, set2):\n",
    "    return (2 * len(set1.intersection(set2))) / (len(set1) + len(set2)) if (len(set1) + len(set2)) > 0 else 0\n",
    "\n",
    "# Funzione per calcolare la corrispondenza per ogni attributo\n",
    "def calculate_similarity(gs_df, ann_df, threshold):\n",
    "    similarity_results = {\n",
    "        'AttributoNomeCompleto': [],\n",
    "        'Jaccard': [],\n",
    "        'Sorensen-Dice': [],\n",
    "        'Corrispondenza': []\n",
    "    }\n",
    "\n",
    "    # Iteriamo su ciascun attributo\n",
    "    for attribute in gs_df['AttributoNomeCompleto'].unique():\n",
    "        # Prendiamo gli insiemi di annotazioni per l'attributo in entrambi gli schemi\n",
    "        gs_annotations = set(gs_df[gs_df['AttributoNomeCompleto'] == attribute]['PrettyName'])\n",
    "        ann_annotations = set(ann_df[ann_df['AttributoNomeCompleto'] == attribute]['PrettyName'])\n",
    "\n",
    "        # Calcoliamo le metriche di similarità\n",
    "        jaccard = jaccard_similarity(gs_annotations, ann_annotations)\n",
    "        dice = sorensen_dice(gs_annotations, ann_annotations)\n",
    "\n",
    "        # Verifica della corrispondenza (Jaccard)\n",
    "        corrispondenza = 1 if jaccard >= threshold else 0\n",
    "\n",
    "        # Aggiungiamo i risultati\n",
    "        similarity_results['AttributoNomeCompleto'].append(attribute)\n",
    "        similarity_results['Jaccard'].append(jaccard)\n",
    "        similarity_results['Sorensen-Dice'].append(dice)\n",
    "        similarity_results['Corrispondenza'].append(corrispondenza)\n",
    "\n",
    "    # Creiamo un DataFrame con i risultati\n",
    "    similarity_df = pd.DataFrame(similarity_results)\n",
    "\n",
    "    return similarity_df\n",
    "\n",
    "\n",
    "\n",
    "# Definiamo una soglia di similarità per la corrispondenza\n",
    "threshold = 0.8\n",
    "\n",
    "# Calcoliamo la similarità tra gli attributi\n",
    "similarity_df = calculate_similarity(GS, annotazioni, threshold)\n",
    "\n",
    "# Visualizziamo i risultati\n",
    "print(similarity_df)\n"
   ],
   "id": "afbd67d391614ac1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 AttributoNomeCompleto   Jaccard  Sorensen-Dice  \\\n",
      "0                    location-location  0.000000       0.000000   \n",
      "1                 location-location_id  0.043478       0.083333   \n",
      "2                   location-address_1  0.040000       0.076923   \n",
      "3                   location-address_2  0.038462       0.074074   \n",
      "4                        location-city  0.080000       0.148148   \n",
      "..                                 ...       ...            ...   \n",
      "69     person-gender_source_concept_id  0.071429       0.133333   \n",
      "70            person-race_source_value  0.081081       0.150000   \n",
      "71       person-race_source_concept_id  0.034483       0.066667   \n",
      "72       person-ethnicity_source_value  0.027027       0.052632   \n",
      "73  person-ethnicity_source_concept_id  0.033333       0.064516   \n",
      "\n",
      "    Corrispondenza  \n",
      "0                0  \n",
      "1                0  \n",
      "2                0  \n",
      "3                0  \n",
      "4                0  \n",
      "..             ...  \n",
      "69               0  \n",
      "70               0  \n",
      "71               0  \n",
      "72               0  \n",
      "73               0  \n",
      "\n",
      "[74 rows x 4 columns]\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Valutazione della Corrispondenza con Precision, Recall e F1",
   "id": "5c62aa58f238e8fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:03:23.428383Z",
     "start_time": "2025-03-27T18:03:23.143402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Funzioni per calcolare TP, FP e FN\n",
    "def true_positives(gs_set, ann_set):\n",
    "    return len(gs_set.intersection(ann_set))\n",
    "\n",
    "def false_positives(gs_set, ann_set):\n",
    "    return len(ann_set - gs_set)\n",
    "\n",
    "def false_negatives(gs_set, ann_set):\n",
    "    return len(gs_set - ann_set)\n",
    "\n",
    "# Funzione per calcolare Precision, Recall e F1-Score\n",
    "def calculate_metrics(gs_set, ann_set):\n",
    "    tp = true_positives(gs_set, ann_set)\n",
    "    fp = false_positives(gs_set, ann_set)\n",
    "    fn = false_negatives(gs_set, ann_set)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Funzione per confrontare gli attributi e calcolare le metriche aggregate\n",
    "def compare_attributes(gs_df, ann_df, threshold=0.5):\n",
    "    comparison_results = {}\n",
    "\n",
    "    for attribute in gs_df['AttributoNomeCompleto'].unique():\n",
    "        # Estrai gli insiemi di annotazioni per l'attributo corrente\n",
    "        gs_annotations = set(gs_df[gs_df['AttributoNomeCompleto'] == attribute]['PrettyName'])\n",
    "        ann_annotations = set(ann_df[ann_df['AttributoNomeCompleto'] == attribute]['PrettyName'])\n",
    "\n",
    "        # Calcolo delle metriche di Precision, Recall e F1-Score\n",
    "        precision, recall, f1 = calculate_metrics(gs_annotations, ann_annotations)\n",
    "\n",
    "        # Salva i risultati\n",
    "        comparison_results[attribute] = {\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1': f1\n",
    "        }\n",
    "\n",
    "    return comparison_results\n",
    "\n",
    "# Supponiamo che gs_df e ann_df siano i tuoi DataFrame contenenti i dati per GS e Annotator X\n",
    "# gs_df = pd.read_csv('gold_standard.csv')\n",
    "# ann_df = pd.read_csv('annotator.csv')\n",
    "\n",
    "# Confrontiamo gli attributi\n",
    "comparison_results = compare_attributes(GS, annotazioni)\n",
    "\n",
    "# Visualizziamo i risultati\n",
    "for attribute, metrics in comparison_results.items():\n",
    "    print(f\"Attributo: {attribute}\")\n",
    "    print(f\"Precision: {metrics['Precision']}\")\n",
    "    print(f\"Recall: {metrics['Recall']}\")\n",
    "    print(f\"F1: {metrics['F1']}\")\n",
    "    print()"
   ],
   "id": "8925638bab3a8ae9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributo: location-location\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: location-location_id\n",
      "Precision: 0.045454545454545456\n",
      "Recall: 0.5\n",
      "F1: 0.08333333333333334\n",
      "\n",
      "Attributo: location-address_1\n",
      "Precision: 0.04\n",
      "Recall: 1.0\n",
      "F1: 0.07692307692307693\n",
      "\n",
      "Attributo: location-address_2\n",
      "Precision: 0.038461538461538464\n",
      "Recall: 1.0\n",
      "F1: 0.07407407407407407\n",
      "\n",
      "Attributo: location-city\n",
      "Precision: 0.08\n",
      "Recall: 1.0\n",
      "F1: 0.14814814814814814\n",
      "\n",
      "Attributo: location-state\n",
      "Precision: 0.041666666666666664\n",
      "Recall: 0.5\n",
      "F1: 0.07692307692307693\n",
      "\n",
      "Attributo: location-zip\n",
      "Precision: 0.09090909090909091\n",
      "Recall: 0.6666666666666666\n",
      "F1: 0.16\n",
      "\n",
      "Attributo: location-county\n",
      "Precision: 0.10526315789473684\n",
      "Recall: 1.0\n",
      "F1: 0.1904761904761905\n",
      "\n",
      "Attributo: location-country\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: location-location_source_value\n",
      "Precision: 0.08695652173913043\n",
      "Recall: 1.0\n",
      "F1: 0.16\n",
      "\n",
      "Attributo: location-latitude\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: location-longitude\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: cost-cost\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: cost-payer_plan_period_id\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: cost-person_id\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: cost-cost_id\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: cost-cost_event_id\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: cost-cost_event_field_concept_id\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: cost-cost_concept_id\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: cost-cost_type_concept_id\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: cost-cost_source_concept_id\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: cost-cost_source_value\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: cost-currency_concept_id\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: cost-incurred_date\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: cost-billed_date\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: cost-paid_date\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: cost-revenue_code_concept_id\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: cost-DRG_concept_id\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: cost-revenue_code_source_value\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: cost-DRG_source_value\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: relationship-relationship\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: relationship-relationship_id\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: relationship-relationship_name\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: relationship-is_the_relationship_hierarchical\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: relationship-define_ancestry\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: relationship-reverse_relationship_id\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: relationship-reverse_concept_id\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: condition_occurrence-condition_occurrence\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: condition_occurrence-condition_occurrence_id\n",
      "Precision: 0.05128205128205128\n",
      "Recall: 0.6666666666666666\n",
      "F1: 0.09523809523809523\n",
      "\n",
      "Attributo: condition_occurrence-person_id\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: condition_occurrence-condition_concept_id\n",
      "Precision: 0.043478260869565216\n",
      "Recall: 0.6666666666666666\n",
      "F1: 0.08163265306122448\n",
      "\n",
      "Attributo: condition_occurrence-condition_start_date\n",
      "Precision: 0.05128205128205128\n",
      "Recall: 1.0\n",
      "F1: 0.09756097560975609\n",
      "\n",
      "Attributo: condition_occurrence-condition_start_datetime\n",
      "Precision: 0.046511627906976744\n",
      "Recall: 0.6666666666666666\n",
      "F1: 0.08695652173913045\n",
      "\n",
      "Attributo: condition_occurrence-condition_end_date\n",
      "Precision: 0.04878048780487805\n",
      "Recall: 1.0\n",
      "F1: 0.09302325581395349\n",
      "\n",
      "Attributo: condition_occurrence-condition_end_datetime\n",
      "Precision: 0.04878048780487805\n",
      "Recall: 0.6666666666666666\n",
      "F1: 0.0909090909090909\n",
      "\n",
      "Attributo: condition_occurrence-condition_type_concept_id\n",
      "Precision: 0.06\n",
      "Recall: 0.75\n",
      "F1: 0.1111111111111111\n",
      "\n",
      "Attributo: condition_occurrence-condition_status_concept_id\n",
      "Precision: 0.058823529411764705\n",
      "Recall: 0.75\n",
      "F1: 0.10909090909090909\n",
      "\n",
      "Attributo: condition_occurrence-stop_reason\n",
      "Precision: 0.05\n",
      "Recall: 0.5\n",
      "F1: 0.09090909090909091\n",
      "\n",
      "Attributo: condition_occurrence-provider_id\n",
      "Precision: 0.025\n",
      "Recall: 0.5\n",
      "F1: 0.047619047619047616\n",
      "\n",
      "Attributo: condition_occurrence-visit_occurrence_id\n",
      "Precision: 0.024390243902439025\n",
      "Recall: 0.3333333333333333\n",
      "F1: 0.04545454545454545\n",
      "\n",
      "Attributo: condition_occurrence-visit_detail_id\n",
      "Precision: 0.023255813953488372\n",
      "Recall: 0.3333333333333333\n",
      "F1: 0.04347826086956522\n",
      "\n",
      "Attributo: condition_occurrence-condition_source_value\n",
      "Precision: 0.041666666666666664\n",
      "Recall: 0.6666666666666666\n",
      "F1: 0.0784313725490196\n",
      "\n",
      "Attributo: condition_occurrence-condition_source_concept_id\n",
      "Precision: 0.05263157894736842\n",
      "Recall: 0.5\n",
      "F1: 0.09523809523809525\n",
      "\n",
      "Attributo: condition_occurrence-condition_status_source_value\n",
      "Precision: 0.057692307692307696\n",
      "Recall: 0.75\n",
      "F1: 0.10714285714285714\n",
      "\n",
      "Attributo: person-person\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: person-person_id\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: person-gender_concept_id\n",
      "Precision: 0.06451612903225806\n",
      "Recall: 0.6666666666666666\n",
      "F1: 0.1176470588235294\n",
      "\n",
      "Attributo: person-year_of_birth\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: person-month_of_birth\n",
      "Precision: 0.029411764705882353\n",
      "Recall: 1.0\n",
      "F1: 0.05714285714285715\n",
      "\n",
      "Attributo: person-day_of_birth\n",
      "Precision: 0.02857142857142857\n",
      "Recall: 1.0\n",
      "F1: 0.05555555555555556\n",
      "\n",
      "Attributo: person-datetime_of_birth\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: person-datetime_of_death\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: person-race_concept_id\n",
      "Precision: 0.058823529411764705\n",
      "Recall: 0.6666666666666666\n",
      "F1: 0.10810810810810811\n",
      "\n",
      "Attributo: person-ethnicity_concept_id\n",
      "Precision: 0.058823529411764705\n",
      "Recall: 0.6666666666666666\n",
      "F1: 0.10810810810810811\n",
      "\n",
      "Attributo: person-location_id\n",
      "Precision: 0.029411764705882353\n",
      "Recall: 0.5\n",
      "F1: 0.05555555555555555\n",
      "\n",
      "Attributo: person-provider_id\n",
      "Precision: 0.030303030303030304\n",
      "Recall: 0.5\n",
      "F1: 0.05714285714285715\n",
      "\n",
      "Attributo: person-care_site_id\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "\n",
      "Attributo: person-person_source_value\n",
      "Precision: 0.05555555555555555\n",
      "Recall: 0.6666666666666666\n",
      "F1: 0.10256410256410256\n",
      "\n",
      "Attributo: person-gender_source_value\n",
      "Precision: 0.06666666666666667\n",
      "Recall: 0.6666666666666666\n",
      "F1: 0.1212121212121212\n",
      "\n",
      "Attributo: person-gender_source_concept_id\n",
      "Precision: 0.07692307692307693\n",
      "Recall: 0.5\n",
      "F1: 0.13333333333333336\n",
      "\n",
      "Attributo: person-race_source_value\n",
      "Precision: 0.08108108108108109\n",
      "Recall: 1.0\n",
      "F1: 0.15\n",
      "\n",
      "Attributo: person-race_source_concept_id\n",
      "Precision: 0.038461538461538464\n",
      "Recall: 0.25\n",
      "F1: 0.06666666666666668\n",
      "\n",
      "Attributo: person-ethnicity_source_value\n",
      "Precision: 0.02857142857142857\n",
      "Recall: 0.3333333333333333\n",
      "F1: 0.05263157894736842\n",
      "\n",
      "Attributo: person-ethnicity_source_concept_id\n",
      "Precision: 0.037037037037037035\n",
      "Recall: 0.25\n",
      "F1: 0.06451612903225806\n",
      "\n"
     ]
    }
   ],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
